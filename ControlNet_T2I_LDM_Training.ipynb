{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17-UrPd8CNZ11yHpF3nbjfUzvW7-YCVVN",
      "authorship_tag": "ABX9TyPSS27BdxHKJZMVpTBibiu8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yang-star-source/ControlNet-In-Latent-Diffusion/blob/main/ControlNet_T2I_LDM_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE"
      ],
      "metadata": {
        "id": "nf7x6MCmNFl-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31m1-rgagUQa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,dropout_prob = 0.0):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        self.norm1 = nn.GroupNorm(32,in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1)\n",
        "        self.norm2 = nn.GroupNorm(32,out_channels)\n",
        "        self.drop = nn.Dropout(dropout_prob)\n",
        "        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1)\n",
        "        self.silu = nn.SiLU()\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels,out_channels,kernel_size=1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x1 = x\n",
        "\n",
        "        x = self.norm1(x)\n",
        "        x = self.silu(x)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.norm2(x)\n",
        "        x = self.silu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.conv2(x)\n",
        "        return x + self.shortcut(x1)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.norm = nn.GroupNorm(32,in_channels)\n",
        "\n",
        "        self.to_q = nn.Linear(in_channels,in_channels)\n",
        "        self.to_k = nn.Linear(in_channels,in_channels)\n",
        "        self.to_v = nn.Linear(in_channels,in_channels)\n",
        "\n",
        "        self.to_out = nn.Linear(in_channels,in_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        residual = x\n",
        "        B,C,H,W = x.shape\n",
        "        x = self.norm(x)\n",
        "        x = x.view(B,C,-1).permute(0,2,1)\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        k = self.to_k(x)\n",
        "        v = self.to_v(x)\n",
        "\n",
        "        attn = torch.bmm(q,k.permute(0,2,1)) # batch matrix multiplication\n",
        "        attn = attn * (C**(-0.5))  # sqrt(dk)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = torch.bmm(attn,v)\n",
        "\n",
        "        out = self.to_out(attn)\n",
        "        out = out.permute(0,2,1).view(B,C,H,W)\n",
        "\n",
        "        return out + residual\n",
        "\n",
        "class MidBlock(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super(MidBlock, self).__init__()\n",
        "        self.res1 = ResNetBlock(in_channels,in_channels)\n",
        "        self.attn1 = AttentionBlock(in_channels)\n",
        "        self.res2 = ResNetBlock(in_channels,in_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.res1(x)\n",
        "        x = self.attn1(x)\n",
        "        x = self.res2(x)\n",
        "        return x\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,has_attn=False):\n",
        "        super(DownBlock, self).__init__()\n",
        "        self.res1 = ResNetBlock(in_channels,out_channels)\n",
        "        self.res2 = ResNetBlock(out_channels,out_channels)\n",
        "        self.down = nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=2,padding=1)\n",
        "        if has_attn:\n",
        "            self.attn = AttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "    def forward(self,x):\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.attn(x)\n",
        "        x = self.down(x)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,in_channels=3,out_channels=4):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.inp = nn.Conv2d(in_channels,64,kernel_size=3,padding=1)\n",
        "        self.down_block = nn.Sequential(\n",
        "            DownBlock(64,128),\n",
        "            DownBlock(128,256),\n",
        "            DownBlock(256,512,has_attn = True)\n",
        "        )\n",
        "        self.bottle = MidBlock(512)\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(32,512),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(512,out_channels*2,kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def reparameterize(self,x):\n",
        "        mean , log_var = torch.chunk(x,2,dim=1)\n",
        "        log_var = torch.clamp(log_var, -30.0, 20.0)\n",
        "        D_kl = 0.5 *(torch.exp(log_var) + mean**2 - log_var - 1)\n",
        "        D_kl = torch.sum(D_kl,dim=[1,2,3]).mean() # Mean is for batch dimention\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mean + eps*std\n",
        "        return z,D_kl\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.inp(x)\n",
        "        x = self.down_block(x)\n",
        "        x = self.bottle(x)\n",
        "        x = self.out(x)\n",
        "        z,D_kl = self.reparameterize(x)\n",
        "        return z,D_kl\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,has_attn=False):\n",
        "        super().__init__()\n",
        "        self.res1 = ResNetBlock(in_channels,out_channels)\n",
        "        self.res2 = ResNetBlock(out_channels,out_channels)\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2), # nearest mode by default\n",
        "            nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1)\n",
        "        )\n",
        "        if has_attn:\n",
        "            self.attn = AttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "    def forward(self,x):\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.attn(x)\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,in_channels=4,out_channels=3):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.inp = nn.Conv2d(in_channels,512,kernel_size=3,padding=1)\n",
        "        self.bottle = MidBlock(512)\n",
        "        self.up_block = nn.Sequential(\n",
        "            UpBlock(512,256,has_attn=True),\n",
        "            UpBlock(256,128),\n",
        "            UpBlock(128,64)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(32,64),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(64,out_channels,kernel_size=3,padding=1)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.inp(x)\n",
        "        x = self.bottle(x)\n",
        "        x = self.up_block(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "class PatchGan(nn.Module):\n",
        "    def __init__(self,in_channels=3):\n",
        "        super(PatchGan, self).__init__()\n",
        "        self.model=nn.Sequential(\n",
        "            nn.Conv2d(in_channels,64,kernel_size=3,stride=2,padding=1),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(64,128,kernel_size=3,stride=2,padding=1),\n",
        "            nn.GroupNorm(32,128),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(128,256,kernel_size=3,stride=2,padding=1),\n",
        "            nn.GroupNorm(32,256),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(256,512,kernel_size=3,stride=2,padding=1),\n",
        "            nn.GroupNorm(32,512),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(512,1,kernel_size=3,stride=1,padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self,in_channels=3,out_channels=4):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = Encoder(in_channels,out_channels)\n",
        "        self.decoder = Decoder(out_channels,in_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        z,D_kl = self.encoder(x)\n",
        "        x = self.decoder(z)\n",
        "        return x,D_kl\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text-Conditioned Diffusion Unet"
      ],
      "metadata": {
        "id": "MyUlQvChNIl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def raw_time_embedding(time , dim):\n",
        "    if not torch.is_tensor(time):\n",
        "        time = torch.tensor(time)\n",
        "\n",
        "    device=time.device\n",
        "    if time.ndim == 0:\n",
        "        time = time.unsqueeze(0).unsqueeze(1)\n",
        "    else: # This will be execute in training since t shape is (B)\n",
        "        time = time.unsqueeze(1)\n",
        "        # (B) -> (B,1)\n",
        "\n",
        "    # important to specify device\n",
        "    i=torch.arange(dim//2,device=device).float()\n",
        "    obj = (time)/(10000**(2*i/dim))\n",
        "    return torch.cat([torch.sin(obj),torch.cos(obj)],dim=1)\n",
        "\n",
        "class TextConditionTimeEmbedding(nn.Module):\n",
        "    def __init__(self,dim):\n",
        "        super().__init__()\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(dim,dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(dim,dim)\n",
        "        )\n",
        "\n",
        "    def forward(self,X):\n",
        "        return self.net(X)\n",
        "\n",
        "class TextConditionDiffusionResNetBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,time_emb_dim,dropout_prob = 0.0 ):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.GroupNorm(32,in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1)\n",
        "        self.norm2 = nn.GroupNorm(32,out_channels)\n",
        "        self.drop = nn.Dropout(dropout_prob)\n",
        "        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1)\n",
        "        self.silu = nn.SiLU()\n",
        "\n",
        "        # purpose of this projection is to match channel dim , before adding to x\n",
        "        self.time_proj = nn.Linear(time_emb_dim,out_channels)\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels,out_channels,kernel_size=1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self,x,time_emb):\n",
        "        x1 = x\n",
        "\n",
        "        x = self.norm1(x)\n",
        "        x = self.silu(x)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # (B,C) -> (B,C,1,1)\n",
        "        # why silu ?\n",
        "        emb = self.time_proj(self.silu(time_emb))\n",
        "        x = x + emb[:, :, None, None]\n",
        "\n",
        "        x = self.norm2(x)\n",
        "        x = self.silu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.conv2(x)\n",
        "        return x + self.shortcut(x1)\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self,channel_dim,multiplier = 4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(channel_dim,channel_dim*multiplier,1),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(channel_dim*multiplier,channel_dim,1)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class CrossAttentionBlock(nn.Module):\n",
        "    def __init__(self,channel_dim , context_dim = 768 ,heads=8):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.head_dim = channel_dim // heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.to_q = nn.Linear(channel_dim,channel_dim,bias=False)\n",
        "        self.to_k = nn.Linear(context_dim,channel_dim,bias=False)\n",
        "        self.to_v = nn.Linear(context_dim,channel_dim,bias=False)\n",
        "\n",
        "        self.to_out = nn.Linear(channel_dim,channel_dim)\n",
        "\n",
        "    def forward(self,x,context):\n",
        "        # x shape : [B,C,H,W]\n",
        "        # context shape : [B,77,768]\n",
        "        B,C,H,W = x.shape\n",
        "\n",
        "        # [B,H*W,C]\n",
        "        x_flat = x.permute(0,2,3,1).reshape(B,H*W,C)\n",
        "\n",
        "        # [B,H*W,C]\n",
        "        q = self.to_q(x_flat)\n",
        "\n",
        "        # [B,77,C]\n",
        "        k = self.to_k(context)\n",
        "        v = self.to_v(context)\n",
        "\n",
        "        # [Batch , head , pixels , head_dim]\n",
        "        q = q.reshape(B,H*W,self.heads,self.head_dim).permute(0,2,1,3)\n",
        "\n",
        "        # [B,8,77,head_dim]\n",
        "        k = k.reshape(B,-1,self.heads,self.head_dim).permute(0,2,1,3)\n",
        "        v = v.reshape(B,-1,self.heads,self.head_dim).permute(0,2,1,3)\n",
        "\n",
        "        # Q shape: [B, 8, H*W ,  head_dim]\n",
        "        # K shape: [B, 8, 77  ,  head_dim] 77 is from CLIP\n",
        "        # Result1: [B, 8, H*W ,  77      ]\n",
        "        # V shape: [B, 8, 77  ,  head_dim]\n",
        "        # Result : [B, 8, H*W , head_dim ]\n",
        "        out = F.scaled_dot_product_attention(q,k,v,dropout_p=0.0,is_causal=False)\n",
        "\n",
        "        # [B,H*W,C] where C = head*head_dim\n",
        "        out = out.permute(0,2,1,3).reshape(B,H*W,C)\n",
        "\n",
        "        # [B,H*W,C]\n",
        "        out = self.to_out(out)\n",
        "\n",
        "        # [B,C,H,W]\n",
        "        out = out.reshape(B,H,W,C).permute(0,3,1,2)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self,channel_dim,context_dim=768):\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm1 = nn.GroupNorm(32,channel_dim)\n",
        "        self.self_attn = CrossAttentionBlock(channel_dim,context_dim=channel_dim)\n",
        "\n",
        "        self.norm2 = nn.GroupNorm(32,channel_dim)\n",
        "        self.cross_attn = CrossAttentionBlock(channel_dim,context_dim=context_dim)\n",
        "\n",
        "        self.norm3 = nn.GroupNorm(32,channel_dim)\n",
        "        self.ff = FeedForwardBlock(channel_dim)\n",
        "\n",
        "    def forward(self,x,context):\n",
        "        B,C,H,W = x.shape\n",
        "\n",
        "        res1 = x\n",
        "        x = self.norm1(x)\n",
        "        flat_x = x.permute(0,2,3,1).reshape(B,H*W,C)\n",
        "        x = res1 + self.self_attn(x,flat_x)\n",
        "\n",
        "        res2 = x\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        x = res2 + self.cross_attn(x,context)\n",
        "\n",
        "        res3 = x\n",
        "        x = self.norm3(x)\n",
        "        x = res3 + self.ff(x)\n",
        "        return x\n",
        "\n",
        "class TextConditionDiffusionDownBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,time_emb_dim,has_attn=False):\n",
        "\n",
        "        self.need_attn = has_attn\n",
        "        super().__init__()\n",
        "        self.res1 = TextConditionDiffusionResNetBlock(in_channels,out_channels,time_emb_dim)\n",
        "        self.res2 = TextConditionDiffusionResNetBlock(out_channels,out_channels,time_emb_dim)\n",
        "        self.down = nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=2,padding=1)\n",
        "        if has_attn:\n",
        "            self.attn = TransformerBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "    def forward(self,x,time_emb,context):\n",
        "        x = self.res1(x,time_emb)\n",
        "        x = self.res2(x,time_emb)\n",
        "        if self.need_attn:\n",
        "            x = self.attn(x,context)\n",
        "        else:\n",
        "            x = self.attn(x)\n",
        "\n",
        "        skip_connection = x\n",
        "\n",
        "        x = self.down(x)\n",
        "        return x , skip_connection\n",
        "\n",
        "class TextConditionDiffusionMidBlock(nn.Module):\n",
        "    def __init__(self,in_channels,time_emb_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.res1 = TextConditionDiffusionResNetBlock(in_channels,in_channels,time_emb_dim)\n",
        "        self.attn = TransformerBlock(in_channels)\n",
        "        self.res2 = TextConditionDiffusionResNetBlock(in_channels,in_channels,time_emb_dim)\n",
        "\n",
        "    def forward(self,x,time_emb,context):\n",
        "        x = self.res1(x,time_emb)\n",
        "        x = self.attn(x,context)\n",
        "        x = self.res2(x,time_emb)\n",
        "        return x\n",
        "\n",
        "class TextConditionDiffusionUpBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,time_emb_dim,has_attn=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.need_attn = has_attn\n",
        "\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(in_channels,in_channels,kernel_size=3,padding=1)\n",
        "        )\n",
        "        # Why input *2 ? Because we have to concatenate the channels\n",
        "        self.res1 = TextConditionDiffusionResNetBlock(in_channels*2,out_channels,time_emb_dim)\n",
        "        self.res2 = TextConditionDiffusionResNetBlock(out_channels,out_channels,time_emb_dim)\n",
        "        if has_attn:\n",
        "            self.attn = TransformerBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "    def forward(self,x,skip_connection,time_emb,context):\n",
        "        x = self.up(x)\n",
        "\n",
        "        # cancatenate at channels dimension\n",
        "        x = torch.cat([x,skip_connection],dim=1)\n",
        "        x = self.res1(x,time_emb)\n",
        "        x = self.res2(x,time_emb)\n",
        "\n",
        "        if self.need_attn:\n",
        "            x = self.attn(x,context)\n",
        "        else:\n",
        "            x = self.attn(x)\n",
        "        return x\n",
        "\n",
        "class TextConditionDiffusionUnet(nn.Module):\n",
        "    def __init__(self,in_channels=4,out_channels=4,time_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.time_dim = time_dim\n",
        "        self.time_embedding = TextConditionTimeEmbedding(time_dim)\n",
        "\n",
        "        self.init_conv = nn.Conv2d(in_channels,64,kernel_size=3,padding=1)\n",
        "\n",
        "        self.down1 = TextConditionDiffusionDownBlock(64,64,time_dim)\n",
        "        self.down2 = TextConditionDiffusionDownBlock(64,128,time_dim)\n",
        "        self.down3 = TextConditionDiffusionDownBlock(128,128,time_dim,has_attn=True)\n",
        "        self.down4 = TextConditionDiffusionDownBlock(128,256,time_dim,has_attn=True)\n",
        "\n",
        "        self.mid = TextConditionDiffusionMidBlock(256,time_dim)\n",
        "\n",
        "        self.up1 = TextConditionDiffusionUpBlock(256,128,time_dim,has_attn=True)\n",
        "        self.up2 = TextConditionDiffusionUpBlock(128,128,time_dim,has_attn=True)\n",
        "        self.up3 = TextConditionDiffusionUpBlock(128,64,time_dim)\n",
        "        self.up4 = TextConditionDiffusionUpBlock(64,64,time_dim)\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(32,64),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(64,out_channels,kernel_size=3,padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self,x,context,t,residual):\n",
        "\n",
        "        resi1,resi2,resi3,resi4,resi5 = residual\n",
        "\n",
        "        t = raw_time_embedding(t,self.time_dim)\n",
        "        emb = self.time_embedding(t)\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        x1 , skip1 = self.down1(x,emb,context)\n",
        "        x2 , skip2 = self.down2(x1,emb,context)\n",
        "        x3 , skip3 = self.down3(x2,emb,context)\n",
        "        x4 , skip4 = self.down4(x3,emb,context)\n",
        "\n",
        "        skip1 = skip1 + resi1\n",
        "        skip2 = skip2 + resi2\n",
        "        skip3 = skip3 + resi3\n",
        "        skip4 = skip4 + resi4\n",
        "\n",
        "        x = self.mid(x4,emb,context)\n",
        "        x = x + resi5\n",
        "\n",
        "        x = self.up1(x,skip4,emb,context)\n",
        "        x = self.up2(x,skip3,emb,context)\n",
        "        x = self.up3(x,skip2,emb,context)\n",
        "        x = self.up4(x,skip1,emb,context)\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ozvfnx4hgcy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ControlNet"
      ],
      "metadata": {
        "id": "CZzBnGULNOUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def zero_conv_layer(in_channels,out_channels):\n",
        "    zero_conv = nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=1,padding=0)\n",
        "    nn.init.zeros_(zero_conv.weight)\n",
        "    nn.init.zeros_(zero_conv.bias)\n",
        "    return zero_conv\n",
        "\n",
        "class ConditionEncoder(nn.Module):\n",
        "    # Out Channels map the init_conv in diffusion\n",
        "    # 256 -> 128 -> 64 -> 32\n",
        "    def __init__(self,in_channels=3,out_channels=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_channels,16,kernel_size=3,stride=1,padding=1),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(16,16,kernel_size=3,stride=2,padding=1),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(16,32,kernel_size=3,stride=2,padding=1),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(32,64,kernel_size=3,stride=2,padding=1),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(64,out_channels,kernel_size=3,stride=1,padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.net(x)\n",
        "        return x\n",
        "\n",
        "class ControlNet(nn.Module):\n",
        "    def __init__(self,DiffusionUnet_copy,in_channels=3,out_channels=4):\n",
        "        super().__init__()\n",
        "        self.encoder = ConditionEncoder()\n",
        "\n",
        "        self.time_dim = DiffusionUnet_copy.time_dim\n",
        "\n",
        "        self.time_emb = copy.deepcopy(DiffusionUnet_copy.time_embedding)\n",
        "        self.init_conv = copy.deepcopy(DiffusionUnet_copy.init_conv)\n",
        "\n",
        "        self.down1 = copy.deepcopy(DiffusionUnet_copy.down1)\n",
        "        self.zero_conv1 = zero_conv_layer(64,64)\n",
        "\n",
        "        self.down2 = copy.deepcopy(DiffusionUnet_copy.down2)\n",
        "        self.zero_conv2 = zero_conv_layer(128,128)\n",
        "\n",
        "        self.down3 = copy.deepcopy(DiffusionUnet_copy.down3)\n",
        "        self.zero_conv3 = zero_conv_layer(128,128)\n",
        "\n",
        "        self.down4 = copy.deepcopy(DiffusionUnet_copy.down4)\n",
        "        self.zero_conv4 = zero_conv_layer(256,256)\n",
        "\n",
        "        self.mid = copy.deepcopy(DiffusionUnet_copy.mid)\n",
        "        self.zero_conv5 = zero_conv_layer(256,256)\n",
        "\n",
        "    def forward(self,x,text_emb,t,condition):\n",
        "        t = raw_time_embedding(t,self.time_dim)\n",
        "        emb = self.time_emb(t)\n",
        "\n",
        "        c = self.encoder(condition)\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        x = x + c\n",
        "\n",
        "        x,skip1 = self.down1(x,emb,text_emb)\n",
        "        out1 = self.zero_conv1(skip1)\n",
        "\n",
        "        x,skip2 = self.down2(x,emb,text_emb)\n",
        "        out2 = self.zero_conv2(skip2)\n",
        "\n",
        "        x,skip3 = self.down3(x,emb,text_emb)\n",
        "        out3 = self.zero_conv3(skip3)\n",
        "\n",
        "        x,skip4 = self.down4(x,emb,text_emb)\n",
        "        out4 = self.zero_conv4(skip4)\n",
        "\n",
        "        x_mid = self.mid(x,emb,text_emb)\n",
        "        out5 = self.zero_conv5(x_mid)\n",
        "\n",
        "        return (out1,out2,out3,out4,out5)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U9xbcgB6lDvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "qDZ6T2QENSNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "latent_dir = \"/content/dataset/LATENTS\"\n",
        "caption_dir = \"/content/dataset/captions_embedding/captions_embedding\"\n",
        "condition_dir = \"/content/dataset/condition\"\n",
        "os.makedirs(latent_dir,exist_ok=True)\n",
        "os.makedirs(caption_dir,exist_ok=True)\n",
        "\n",
        "repo_id = \"ziyang06315/cats_images_dataset\"\n",
        "LATENTS_ZIP = hf_hub_download(repo_id=repo_id,filename=\"LATENTS.zip\",repo_type=\"dataset\",local_dir=latent_dir)\n",
        "TEXT_EMB_ZIP = hf_hub_download(repo_id=repo_id,filename=\"captions_embedding.zip\",repo_type=\"dataset\",local_dir=caption_dir)\n",
        "CAPTION_ZIP = hf_hub_download(repo_id=repo_id,filename=\"Canny.zip\",repo_type=\"dataset\",local_dir=condition_dir)\n",
        "\n",
        "os.system(f\"unzip -q {LATENTS_ZIP} -d {latent_dir}\")\n",
        "os.system(f\"unzip -q {TEXT_EMB_ZIP} -d {caption_dir}\")\n",
        "os.system(f\"unzip -q {CAPTION_ZIP} -d {condition_dir}\")\n",
        "print(\"Finish unzip\")\n",
        "\n"
      ],
      "metadata": {
        "id": "J-Mp7ss1ghQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "latent_dir  = \"/content/dataset/LATENTS/LATENTS\"\n",
        "caption_dir = \"/content/dataset/captions_embedding/captions_embedding\"\n",
        "print(len(os.listdir(latent_dir)))\n",
        "print(len(os.listdir(caption_dir)))\n",
        "\n",
        "junk_files = [file for file in os.listdir(caption_dir) if not file.endswith(\".pt\")]\n",
        "if len(junk_files) > 0:\n",
        "    print(f\"Found {len(junk_files)} junk files\")\n",
        "    for file in junk_files:\n",
        "        full_path = os.path.join(caption_dir,file)\n",
        "        if os.path.isfile(full_path):\n",
        "            os.remove(full_path) # delete files\n",
        "        else:\n",
        "            shutil.rmtree(full_path) # delete folder\n",
        "else:\n",
        "    print(\"No junk files found\")\n",
        "\n",
        "print(len(os.listdir(latent_dir)))\n",
        "print(len(os.listdir(caption_dir)))"
      ],
      "metadata": {
        "id": "0VmlsMy_hcJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "W3SJrof-Nauk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from natsort import natsorted\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/VAE_Training/ControlNet_Checkpoints/ControlNet_T2I_LDM\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "class ControlNetDataset(Dataset):\n",
        "    def __init__(self,latent_dir,text_emb_dir,canny_dir):\n",
        "        self.text_emb_dir = text_emb_dir\n",
        "        self.latent_dir = latent_dir\n",
        "        self.canny_dir = canny_dir\n",
        "\n",
        "        self.latent_files = natsorted(os.listdir(latent_dir))\n",
        "        self.canny_files = natsorted(os.listdir(canny_dir))\n",
        "        self.text_emb_files = natsorted(os.listdir(text_emb_dir))\n",
        "\n",
        "        assert len(self.latent_files) == len(self.canny_files) , \"Mismatch!\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.latent_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        latent_path = os.path.join(self.latent_dir,self.latent_files[index])\n",
        "        latent = torch.load(latent_path)\n",
        "\n",
        "        text_emb_path = os.path.join(self.text_emb_dir,self.text_emb_files[index])\n",
        "        text_emb = torch.load(text_emb_path)\n",
        "\n",
        "        canny_path = os.path.join(self.canny_dir,self.canny_files[index])\n",
        "        canny = cv2.imread(canny_path)\n",
        "        canny = cv2.cvtColor(canny,cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        canny = cv2.resize(canny, (256, 256), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # (H,W,C) -> (C,H,W) | [0,255] -> [0.0,1.0]\n",
        "        canny = torch.from_numpy(canny).permute(2,0,1).float()/255.0\n",
        "        return latent,text_emb,canny\n",
        "\n",
        "LATENTS_DIR = \"/content/dataset/LATENTS/LATENTS\"\n",
        "TEXT_EMB_DIR = \"/content/dataset/captions_embedding/captions_embedding\"\n",
        "CANNY_DIR = \"/content/dataset/condition/Canny\"\n",
        "\n",
        "dataset = ControlNetDataset(LATENTS_DIR,TEXT_EMB_DIR,CANNY_DIR)\n",
        "\n",
        "dataloader = DataLoader(dataset,\n",
        "                        batch_size=32,\n",
        "                        shuffle=True,\n",
        "                        num_workers=2,\n",
        "                        pin_memory=True)\n",
        "\n",
        "sample_latents, sample_text_emb ,sample_canny = next(iter(dataloader))\n",
        "print(f\"Latents Batch Shape: {sample_latents.shape}\")\n",
        "print(f\"Text Embedding Batch Shape: {sample_text_emb.shape}\")\n",
        "print(f\"Canny Batch Shape:   {sample_canny.shape}\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "repo_id = \"ziyang06315/latent_diffusion_from_scratch\"\n",
        "UNET_PATH = hf_hub_download(repo_id, filename=\"TextConditionUnetEpoch_800.pth\")\n",
        "'''\n",
        "VAE_CHECKPOINT = hf_hub_download(repo_id, filename=\"checkpoint.pth\")\n",
        "\n",
        "\n",
        "vae=VAE().to(DEVICE)\n",
        "checkpoint = torch.load(VAE_CHECKPOINT,map_location=DEVICE)\n",
        "vae.load_state_dict(checkpoint['vae_state_dict'])\n",
        "vae.eval()\n",
        "'''\n",
        "\n",
        "unet = TextConditionDiffusionUnet().to(DEVICE)\n",
        "unet = torch.compile(unet)\n",
        "unet.load_state_dict(torch.load(UNET_PATH))\n",
        "unet.eval()\n",
        "for param in unet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "controlnet = ControlNet(unet).to(DEVICE)\n",
        "controlnet.train()\n",
        "\n",
        "optimizer = AdamW(controlnet.parameters(),lr=1e-4)\n",
        "\n",
        "beta_start = 0.0001\n",
        "beta_end = 0.02\n",
        "num_timesteps = 1000\n",
        "\n",
        "betas = torch.linspace(beta_start, beta_end, num_timesteps,device=DEVICE)\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "\n",
        "EPOCH = 100\n",
        "for epoch in range(EPOCH):\n",
        "    epoch_loss = 0.0\n",
        "    progress_bar = tqdm(dataloader,desc=f\"Epoch {epoch+1}/{EPOCH}\")\n",
        "    for latent,text_emb,canny in progress_bar:\n",
        "        latent = latent.to(DEVICE)\n",
        "        text_emb = text_emb.to(DEVICE)\n",
        "        canny = canny.to(DEVICE)\n",
        "\n",
        "        noise = torch.randn_like(latent)\n",
        "        t = torch.randint(0,num_timesteps,(latent.shape[0],),device=DEVICE)\n",
        "\n",
        "        sqrt_alpha_cumprod = sqrt_alphas_cumprod[t][:,None,None,None]\n",
        "        sqrt_one_minus_alpha_cumprod = sqrt_one_minus_alphas_cumprod[t][:,None,None,None]\n",
        "\n",
        "        noisy_latents = sqrt_alpha_cumprod * latent + sqrt_one_minus_alpha_cumprod * noise\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        residuals = controlnet(noisy_latents,text_emb,t,canny)\n",
        "        noise_pred = unet(noisy_latents,text_emb,t,residuals)\n",
        "\n",
        "        loss = F.mse_loss(noise_pred,noise)\n",
        "        epoch_loss+=loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1} Average Loss: {average_loss:.6f}\")\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        SAVE_PATH = os.path.join(SAVE_DIR,f\"ControlNetT2ILDM_{epoch+1}.pth\")\n",
        "        torch.save(controlnet.state_dict(),SAVE_PATH)\n",
        "        print(f\"Saved at {SAVE_PATH}\")\n",
        "\n",
        "print(\"Training Finished\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mv-zHlTsi4gZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}